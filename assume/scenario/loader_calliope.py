# SPDX-FileCopyrightText: ASSUME Developers
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import importlib
from pathlib import Path

# must be installed for import path to work
import calliope
import pandas as pd
import yaml

from assume.common.forecasts import NaiveForecast
from assume.world import World

# model = calliope.Model("path/to/model.yaml", override_dict=override_dict)
model = calliope.examples.urban_scale()


def read_csv(base_path, filename):
    return pd.read_csv(
        base_path + "/" + filename,
        date_format="%Y-%m-%d_%H:%M:%S",
        sep=";",
        header=None,
        names=["time", "load"],
        index_col="time",
    )["load"]


input_path = Path(importlib.resources.files("calliope") / "example_models")
name = "urban_scale"
model_path = input_path / name
# model = calliope.Model("path/to/model.yaml", override_dict=override_dict)

model = calliope.examples.urban_scale()

with open(model_path) as f:
    # TODO also load imported files in yaml
    model_data = yaml.safe_load(f)
    # TODO replace data_sources with actual data

    time_subset = model_data["config"]["init"]["time_subset"]

    start, end = time_subset


override_dict = {"config": {"init": {"time_subset": ["2005-07-07", "2005-07-08"]}}}


def update_dict_keys(initial_dict: dict, override_dict: dict):
    for key, value in override_dict.items():
        if isinstance(value, dict):
            if key in initial_dict:
                update_dict_keys(initial_dict[key], value)
            else:
                initial_dict[key] = value
        else:
            initial_dict[key] = value


async def load_calliope_async(
    world: World, name: str, input_path: Path, override_dict: dict = {}
):
    model_path = input_path / name
    with open(model_path / "model.yaml") as f:
        model_data = yaml.safe_load(f)

    for path in model_data["import"]:
        with open(model_path / path) as f:
            import_dict = yaml.safe_load(f)
            update_dict_keys(model_data, import_dict)

    update_dict_keys(model_data, override_dict)

    if model_data["config"]["build"]["mode"] == "plan":
        raise Exception("Only operate is currently supported in ASSUME")

    time_subset = model_data["config"]["init"]["time_subset"]

    start, end = time_subset

    save_interval = 48

    index = pd.date_range(start=start, end=end, freq="1h")

    # TODO replace data_sources with actual data

    await world.setup(
        start=start,
        end=end,
        save_frequency_hours=save_interval,
        simulation_id=name,
        index=index,
    )

    # TODO add market operator

    agents = []

    for source_name, data_source in model_data["data_sources"].items():
        if data_source["source"]:
            source_path = data_source["source"]
            if data_source.get("drop"):
                skip = 1
                header = 0
            else:
                skip = 0
                if isinstance(data_source["columns"], list):
                    header = len(data_source["columns"])
                else:
                    header = 0
            df = pd.read_csv(
                model_path / source_path,
                header=header,
                skiprows=skip,
                parse_dates=True,
                index_col=0,
            )
        else:
            df = pd.DataFrame(index=index)

        # TODO don't know what this is for
        # data_source now has the actual tech
        tech = data_source["add_dimensions"]["techs"]

        translate_dict = {}  # TODO

        # amiris annotation
        #         {
        #     "NUCLEAR": "nuclear",
        #     "LIGNITE": "lignite",
        #     "HARD_COAL": "hard coal",
        #     "NATURAL_GAS": "natural gas",
        #     "OIL": "oil",
        #     "HYDROGEN": "hydrogen",
        #     "Biogas": "biomass",
        #     "PV": "solar",
        #     "WindOn": "wind_onshore",
        #     "WindOff": "wind_offshore",
        #     "RunOfRiver": "hydro",
        #     "Other": "other",
        # }

        world.unit_types.keys()

        for column in df.columns:
            tech, node = column

        tech_dict = model_data["techs"][tech]

        match tech_dict["base_tech"]:
            case "supply":
                # here we know exactly which parameters exist
                pass
            case "conversion":
                max_power = tech_dict["flow_cap_max"]["data"]
                efficiency = tech_dict["flow_out_eff"]["data"]
                # TODO durch m√∂gliche colums iterieren
                agents.append(
                    {
                        "agent": df.columns[0] + "random",
                        "node": node,
                        "technology": actual_tech,
                    }
                )
            case "demand":
                max_power = df["demand_heat", "X1"].max()
                availability = df["demand_heat", "X1"] / max_power
                min_power = 0
                for column in df.columns:
                    tech, node = column
                actual_tech, product_type = translate_dict.get(tech)
                # TODO check if product type is demand or actual tech
                agents.append(
                    {
                        "agent": df.columns,
                        "node": node,
                        "technology": actual_tech,
                    }
                )

                world.add_unit_operator(source_name)
                world.add_unit(
                    f"{source_name}1",
                    "demand",  # TODO - power_plant?
                    source_name,  # units_operator name
                    # the unit_params have no hints
                    {
                        "min_power": min_power,
                        "max_power": max_power,
                        "bidding_strategies": {"EOM": "naive_eom"},
                        "technology": "demand",
                        "node": "location1",
                    },
                    NaiveForecast(
                        index, demand=df["demand_heat", "X1"], availability=1
                    ),  # hier zeitreihen hinterlegen
                )


if __name__ == "__main__":
    db_uri = "postgresql://assume:assume@localhost:5432/assume"
    world = World(database_uri=db_uri)
    scenario = "urban_scale"

    base_path = Path(importlib.resources.files("calliope") / "example_models")
    world.loop.run_until_complete(
        load_calliope_async(
            world,
            scenario,
            base_path,
        )
    )
    print(f"did load {scenario} - now simulating")
    world.run()
